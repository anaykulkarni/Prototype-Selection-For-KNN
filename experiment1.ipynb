{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from MnistDataloader import MnistDataloader\n",
    "from oneNNClassifier import oneNNClassifier\n",
    "from utilities import random_sample\n",
    "from os.path  import join\n",
    "import timeit\n",
    "from sklearn.metrics import accuracy_score\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = './dataset/'\n",
    "training_images_filepath = join(input_path, 'train-images-idx3-ubyte')\n",
    "training_labels_filepath = join(input_path, 'train-labels-idx1-ubyte')\n",
    "test_images_filepath = join(input_path, 't10k-images-idx3-ubyte')\n",
    "test_labels_filepath = join(input_path, 't10k-labels-idx1-ubyte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataloader = MnistDataloader(training_images_filepath, training_labels_filepath, test_images_filepath, test_labels_filepath)\n",
    "(x_train, y_train), (x_test, y_test) = mnist_dataloader.load_data()\n",
    "x_train = [np.hstack(x).astype(np.float32) for x in x_train]\n",
    "x_test = [np.hstack(x).astype(np.float32) for x in x_test]\n",
    "y_train = np.array(y_train, np.float32)\n",
    "y_test = np.array(y_test, np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.float32, numpy.float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train[0][1]), type(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 60000, 10000, 10000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train), len(y_train), len(x_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 5.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[0]), y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_data = {}\n",
    "storage = {}\n",
    "size = len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{60000: {'time': 1287.285123041831, 'accuracy': 0.9691}}\n"
     ]
    }
   ],
   "source": [
    "model = oneNNClassifier(x_train, y_train)\n",
    "elapsed_time = timeit.timeit(lambda: model.predict(x_test, size=size, storage=storage), \n",
    "                             number=1)\n",
    "accuracy = accuracy_score(y_test, storage[size])\n",
    "\n",
    "execution_data[size] = {\"time\": elapsed_time, \"accuracy\": accuracy}\n",
    "print(execution_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"execution_data_baseline.json\", \"w\") as file:\n",
    "    json.dump(execution_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0 :\n",
      "Sample size: 100, Accuracy: 0.70, Execution time: 1.9583 seconds\n",
      "Sample size: 500, Accuracy: 0.85, Execution time: 9.6544 seconds\n",
      "Sample size: 1000, Accuracy: 0.88, Execution time: 19.2811 seconds\n",
      "Sample size: 2000, Accuracy: 0.91, Execution time: 39.5965 seconds\n",
      "Sample size: 5000, Accuracy: 0.93, Execution time: 107.5737 seconds\n",
      "Sample size: 10000, Accuracy: 0.95, Execution time: 210.7949 seconds\n",
      "Trial 1 :\n",
      "Sample size: 100, Accuracy: 0.69, Execution time: 1.4940 seconds\n",
      "Sample size: 500, Accuracy: 0.85, Execution time: 7.2783 seconds\n",
      "Sample size: 1000, Accuracy: 0.88, Execution time: 14.4138 seconds\n",
      "Sample size: 2000, Accuracy: 0.91, Execution time: 29.0696 seconds\n",
      "Sample size: 5000, Accuracy: 0.94, Execution time: 78.5522 seconds\n",
      "Sample size: 10000, Accuracy: 0.95, Execution time: 176.5432 seconds\n",
      "Trial 2 :\n",
      "Sample size: 100, Accuracy: 0.72, Execution time: 1.4962 seconds\n",
      "Sample size: 500, Accuracy: 0.86, Execution time: 7.2991 seconds\n",
      "Sample size: 1000, Accuracy: 0.89, Execution time: 16.5107 seconds\n",
      "Sample size: 2000, Accuracy: 0.92, Execution time: 32.6669 seconds\n",
      "Sample size: 5000, Accuracy: 0.94, Execution time: 80.1396 seconds\n",
      "Sample size: 10000, Accuracy: 0.95, Execution time: 156.6312 seconds\n",
      "Trial 3 :\n",
      "Sample size: 100, Accuracy: 0.72, Execution time: 1.3911 seconds\n",
      "Sample size: 500, Accuracy: 0.85, Execution time: 6.6330 seconds\n",
      "Sample size: 1000, Accuracy: 0.89, Execution time: 13.3128 seconds\n",
      "Sample size: 2000, Accuracy: 0.91, Execution time: 26.5891 seconds\n",
      "Sample size: 5000, Accuracy: 0.93, Execution time: 70.2856 seconds\n",
      "Sample size: 10000, Accuracy: 0.95, Execution time: 151.1434 seconds\n",
      "Trial 4 :\n",
      "Sample size: 100, Accuracy: 0.68, Execution time: 1.3873 seconds\n",
      "Sample size: 500, Accuracy: 0.84, Execution time: 6.6627 seconds\n",
      "Sample size: 1000, Accuracy: 0.89, Execution time: 13.3132 seconds\n",
      "Sample size: 2000, Accuracy: 0.91, Execution time: 26.5542 seconds\n",
      "Sample size: 5000, Accuracy: 0.93, Execution time: 69.8406 seconds\n",
      "Sample size: 10000, Accuracy: 0.95, Execution time: 150.8263 seconds\n"
     ]
    }
   ],
   "source": [
    "# Uniformly weighted and Randomly sampled train data\n",
    "\n",
    "# sample_sizes = [1, 10, 20, 30, 40, 50]\n",
    "sample_sizes = [100, 500, 1000, 2000, 5000, 10000]\n",
    "storage = {} \n",
    "execution_data = {}\n",
    "\n",
    "for trial in range(5):\n",
    "    execution_data[trial] = []\n",
    "    print('Trial', trial, ':')\n",
    "    for M in sample_sizes:\n",
    "        x_sample, y_sample = random_sample(M, x_train, y_train)\n",
    "\n",
    "        model = oneNNClassifier(x_sample, y_sample)\n",
    "        elapsed_time = timeit.timeit(lambda: model.predict(x_test, size=M, storage=storage), \n",
    "                                number=1)\n",
    "        accuracy = accuracy_score(y_test, storage[M])\n",
    "\n",
    "        print(f\"Sample size: {M}, Accuracy: {accuracy:.2f}, Execution time: {elapsed_time:.4f} seconds\")\n",
    "        execution_data[trial].append({\"sample_size\": M, \"time\": elapsed_time, \"accuracy\": accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"execution_data_random.json\", \"w\") as file:\n",
    "    json.dump(execution_data, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
