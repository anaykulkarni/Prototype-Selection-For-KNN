{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from MnistDataloader import MnistDataloader\n",
    "from oneNNClassifier import oneNNClassifier\n",
    "from utilities import random_sample\n",
    "from os.path  import join\n",
    "import timeit\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = './dataset/'\n",
    "training_images_filepath = join(input_path, 'train-images-idx3-ubyte')\n",
    "training_labels_filepath = join(input_path, 'train-labels-idx1-ubyte')\n",
    "test_images_filepath = join(input_path, 't10k-images-idx3-ubyte')\n",
    "test_labels_filepath = join(input_path, 't10k-labels-idx1-ubyte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataloader = MnistDataloader(training_images_filepath, training_labels_filepath, test_images_filepath, test_labels_filepath)\n",
    "(x_train, y_train), (x_test, y_test) = mnist_dataloader.load_data()\n",
    "x_train = np.array([np.hstack(x).astype(np.float32) for x in x_train])\n",
    "x_test = np.array([np.hstack(x).astype(np.float32) for x in x_test])\n",
    "y_train = np.array(y_train, np.float32)\n",
    "y_test = np.array(y_test, np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.float32, numpy.float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train[0][1]), type(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 60000, 10000, 10000, 784, 5.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train), len(y_train), len(x_test), len(y_test), len(x_train[0]), y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_pca(X, n_components=50):\n",
    "    pca = PCA(n_components=n_components, random_state=42)\n",
    "    X_reduced = pca.fit_transform(X)\n",
    "    return X_reduced, pca\n",
    "\n",
    "# Selects k prototypes from X_reduced using the farthest-point (k-center) clustering approach. \n",
    "# Returns the indices of selected prototypes in the original array.\n",
    "def farthest_point_clustering(X_reduced, k):\n",
    "    n_samples = X_reduced.shape[0]\n",
    "    \n",
    "    # Pick a random point as the first center\n",
    "    first_index = np.random.randint(n_samples)\n",
    "    selected_indices = [first_index]\n",
    "    \n",
    "    # Distances[i] will store the distance of point i from the nearest selected center\n",
    "    distances = np.full(n_samples, np.inf)\n",
    "    \n",
    "    # Update initial distances\n",
    "    distances = np.linalg.norm(X_reduced - X_reduced[first_index], axis=1)\n",
    "\n",
    "    # Iteratively add new centers\n",
    "    for _ in range(k - 1):\n",
    "        # Find the point that is farthest from any selected center\n",
    "        next_index = np.argmax(distances)\n",
    "        selected_indices.append(next_index)\n",
    "\n",
    "        # Update the distances to the newly added center\n",
    "        new_center = X_reduced[next_index]\n",
    "        dist_to_new_center = np.linalg.norm(X_reduced - new_center, axis=1)\n",
    "        \n",
    "        # For each point, keep the distance to the closest center\n",
    "        distances = np.minimum(distances, dist_to_new_center)\n",
    "\n",
    "    return selected_indices\n",
    "\n",
    "# Performs PCA for dimensionality reduction.\n",
    "# For each class, runs farthest-point clustering on that class's points to select prototypes.\n",
    "def select_k_centers_per_class(X, y, k_per_class=10, n_components=50):\n",
    "\n",
    "    # Reduce dimensionality of the entire dataset\n",
    "    X_reduced, pca_model = perform_pca(X, n_components=n_components)\n",
    "    \n",
    "    unique_classes = np.unique(y)\n",
    "    selected_indices = []\n",
    "\n",
    "    for c in unique_classes:\n",
    "        # Extract the indices for class c\n",
    "        class_indices = np.where(y == c)[0]\n",
    "        X_c_reduced = X_reduced[class_indices]\n",
    "\n",
    "        # Run farthest-point clustering in the reduced space\n",
    "        k_center_indices = farthest_point_clustering(X_c_reduced, k_per_class)\n",
    "\n",
    "        # Map back to overall dataset indices\n",
    "        selected_indices_c = class_indices[k_center_indices]\n",
    "        selected_indices.extend(selected_indices_c)\n",
    "    \n",
    "    return selected_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size: 100, Accuracy: 0.51, Execution time: 1.5603 seconds\n",
      "Sample size: 500, Accuracy: 0.78, Execution time: 7.1632 seconds\n",
      "Sample size: 1000, Accuracy: 0.84, Execution time: 14.2292 seconds\n",
      "Sample size: 2000, Accuracy: 0.88, Execution time: 28.4232 seconds\n",
      "Sample size: 5000, Accuracy: 0.92, Execution time: 70.8696 seconds\n",
      "Sample size: 10000, Accuracy: 0.94, Execution time: 141.1148 seconds\n",
      "Sample size: 100, Accuracy: 0.50, Execution time: 1.5245 seconds\n",
      "Sample size: 500, Accuracy: 0.74, Execution time: 7.1687 seconds\n",
      "Sample size: 1000, Accuracy: 0.81, Execution time: 14.1669 seconds\n",
      "Sample size: 2000, Accuracy: 0.87, Execution time: 28.1235 seconds\n",
      "Sample size: 5000, Accuracy: 0.91, Execution time: 70.3498 seconds\n",
      "Sample size: 10000, Accuracy: 0.94, Execution time: 141.0023 seconds\n",
      "Sample size: 100, Accuracy: 0.46, Execution time: 1.5222 seconds\n",
      "Sample size: 500, Accuracy: 0.66, Execution time: 7.1536 seconds\n",
      "Sample size: 1000, Accuracy: 0.76, Execution time: 14.0480 seconds\n",
      "Sample size: 2000, Accuracy: 0.83, Execution time: 28.1549 seconds\n",
      "Sample size: 5000, Accuracy: 0.89, Execution time: 70.4650 seconds\n",
      "Sample size: 10000, Accuracy: 0.93, Execution time: 141.3080 seconds\n",
      "Sample size: 100, Accuracy: 0.49, Execution time: 1.5053 seconds\n",
      "Sample size: 500, Accuracy: 0.65, Execution time: 7.1827 seconds\n",
      "Sample size: 1000, Accuracy: 0.71, Execution time: 14.2271 seconds\n",
      "Sample size: 2000, Accuracy: 0.80, Execution time: 28.1252 seconds\n",
      "Sample size: 5000, Accuracy: 0.88, Execution time: 69.9894 seconds\n",
      "Sample size: 10000, Accuracy: 0.92, Execution time: 141.3311 seconds\n",
      "Sample size: 100, Accuracy: 0.47, Execution time: 1.4962 seconds\n",
      "Sample size: 500, Accuracy: 0.60, Execution time: 7.1022 seconds\n",
      "Sample size: 1000, Accuracy: 0.70, Execution time: 14.1379 seconds\n",
      "Sample size: 2000, Accuracy: 0.76, Execution time: 28.2916 seconds\n",
      "Sample size: 5000, Accuracy: 0.85, Execution time: 70.4489 seconds\n",
      "Sample size: 10000, Accuracy: 0.91, Execution time: 140.4198 seconds\n",
      "Sample size: 100, Accuracy: 0.45, Execution time: 1.4788 seconds\n",
      "Sample size: 500, Accuracy: 0.55, Execution time: 7.0842 seconds\n",
      "Sample size: 1000, Accuracy: 0.65, Execution time: 14.1311 seconds\n",
      "Sample size: 2000, Accuracy: 0.73, Execution time: 28.1035 seconds\n",
      "Sample size: 5000, Accuracy: 0.84, Execution time: 70.5239 seconds\n",
      "Sample size: 10000, Accuracy: 0.90, Execution time: 141.5187 seconds\n",
      "Sample size: 100, Accuracy: 0.47, Execution time: 1.4443 seconds\n",
      "Sample size: 500, Accuracy: 0.55, Execution time: 7.1022 seconds\n",
      "Sample size: 1000, Accuracy: 0.67, Execution time: 14.1200 seconds\n",
      "Sample size: 2000, Accuracy: 0.73, Execution time: 28.3157 seconds\n",
      "Sample size: 5000, Accuracy: 0.83, Execution time: 70.2854 seconds\n",
      "Sample size: 10000, Accuracy: 0.90, Execution time: 140.9600 seconds\n",
      "Sample size: 100, Accuracy: 0.49, Execution time: 1.4374 seconds\n",
      "Sample size: 500, Accuracy: 0.57, Execution time: 7.0909 seconds\n",
      "Sample size: 1000, Accuracy: 0.63, Execution time: 14.1283 seconds\n",
      "Sample size: 2000, Accuracy: 0.73, Execution time: 28.3257 seconds\n",
      "Sample size: 5000, Accuracy: 0.82, Execution time: 71.7652 seconds\n",
      "Sample size: 10000, Accuracy: 0.90, Execution time: 141.8803 seconds\n",
      "Sample size: 100, Accuracy: 0.49, Execution time: 1.4389 seconds\n",
      "Sample size: 500, Accuracy: 0.57, Execution time: 7.0839 seconds\n",
      "Sample size: 1000, Accuracy: 0.64, Execution time: 14.1799 seconds\n",
      "Sample size: 2000, Accuracy: 0.72, Execution time: 28.3085 seconds\n",
      "Sample size: 5000, Accuracy: 0.82, Execution time: 70.3794 seconds\n",
      "Sample size: 10000, Accuracy: 0.89, Execution time: 141.3691 seconds\n"
     ]
    }
   ],
   "source": [
    "# train set is sampled using M/10 prototypes per class\n",
    "\n",
    "# sample_sizes = [10, 20, 30, 40, 50]\n",
    "reduced_dims = [3, 4, 8, 16, 64, 128, 256, 512, 784]\n",
    "sample_sizes = [100, 500, 1000, 2000, 5000, 10000]\n",
    "storage = {} \n",
    "execution_data = {d:[] for d in reduced_dims}\n",
    "\n",
    "for dim in reduced_dims:\n",
    "    for M in sample_sizes:\n",
    "        # Sample prototypes\n",
    "        selected_idxs = select_k_centers_per_class(x_train, y_train, k_per_class= int(M/10), n_components=dim)\n",
    "        x_sample, y_sample = x_train[selected_idxs], y_train[selected_idxs]\n",
    "\n",
    "        # Model\n",
    "        model = oneNNClassifier(x_sample, y_sample)\n",
    "        \n",
    "        # Timing\n",
    "        elapsed_time = timeit.timeit(lambda: model.predict(x_test, size=M, storage=storage, weighted=False), \n",
    "                                number=1)\n",
    "        # Accuracy\n",
    "        accuracy = accuracy_score(y_test, storage[M])\n",
    "\n",
    "        print(f\"Sample size: {M}, Accuracy: {accuracy:.2f}, Execution time: {elapsed_time:.4f} seconds\")\n",
    "        # Store Data\n",
    "        execution_data[dim].append({\"sample_size\": M, \"time\": elapsed_time, \"accuracy\": accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"execution_data_farthest_point_clustering.json\", \"w\") as file:\n",
    "    json.dump(execution_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard = StandardScaler()\n",
    "x_train_scaled = standard.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax = MinMaxScaler()\n",
    "x_train_scaled = minmax.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction Dimension: 3\n",
      "\tSample size: 100, Accuracy: 0.45, Execution time: 1.7318 seconds\n",
      "\tSample size: 500, Accuracy: 0.63, Execution time: 7.8626 seconds\n",
      "\tSample size: 1000, Accuracy: 0.64, Execution time: 16.0565 seconds\n",
      "\tSample size: 2000, Accuracy: 0.58, Execution time: 32.7266 seconds\n",
      "\tSample size: 5000, Accuracy: 0.64, Execution time: 88.2672 seconds\n",
      "\tSample size: 10000, Accuracy: 0.68, Execution time: 173.6080 seconds\n",
      "Reduction Dimension: 4\n",
      "\tSample size: 100, Accuracy: 0.54, Execution time: 1.8358 seconds\n",
      "\tSample size: 500, Accuracy: 0.64, Execution time: 8.3858 seconds\n",
      "\tSample size: 1000, Accuracy: 0.63, Execution time: 16.7313 seconds\n",
      "\tSample size: 2000, Accuracy: 0.57, Execution time: 33.4350 seconds\n",
      "\tSample size: 5000, Accuracy: 0.70, Execution time: 83.6004 seconds\n",
      "\tSample size: 10000, Accuracy: 0.65, Execution time: 172.7699 seconds\n",
      "Reduction Dimension: 8\n",
      "\tSample size: 100, Accuracy: 0.37, Execution time: 1.7950 seconds\n",
      "\tSample size: 500, Accuracy: 0.44, Execution time: 8.3558 seconds\n",
      "\tSample size: 1000, Accuracy: 0.52, Execution time: 16.8963 seconds\n",
      "\tSample size: 2000, Accuracy: 0.58, Execution time: 34.8564 seconds\n",
      "\tSample size: 5000, Accuracy: 0.62, Execution time: 88.1942 seconds\n",
      "\tSample size: 10000, Accuracy: 0.65, Execution time: 175.0821 seconds\n",
      "Reduction Dimension: 16\n",
      "\tSample size: 100, Accuracy: 0.32, Execution time: 1.9067 seconds\n",
      "\tSample size: 500, Accuracy: 0.47, Execution time: 8.5613 seconds\n",
      "\tSample size: 1000, Accuracy: 0.44, Execution time: 16.9229 seconds\n",
      "\tSample size: 2000, Accuracy: 0.54, Execution time: 33.9407 seconds\n",
      "\tSample size: 5000, Accuracy: 0.62, Execution time: 83.9486 seconds\n",
      "\tSample size: 10000, Accuracy: 0.65, Execution time: 170.3681 seconds\n",
      "Reduction Dimension: 64\n",
      "\tSample size: 100, Accuracy: 0.33, Execution time: 1.8183 seconds\n",
      "\tSample size: 500, Accuracy: 0.36, Execution time: 8.5280 seconds\n",
      "\tSample size: 1000, Accuracy: 0.39, Execution time: 16.8487 seconds\n",
      "\tSample size: 2000, Accuracy: 0.49, Execution time: 33.6785 seconds\n",
      "\tSample size: 5000, Accuracy: 0.58, Execution time: 85.0844 seconds\n",
      "\tSample size: 10000, Accuracy: 0.60, Execution time: 164.9511 seconds\n",
      "Reduction Dimension: 128\n",
      "\tSample size: 100, Accuracy: 0.41, Execution time: 1.6697 seconds\n",
      "\tSample size: 500, Accuracy: 0.35, Execution time: 7.6788 seconds\n",
      "\tSample size: 1000, Accuracy: 0.40, Execution time: 15.1673 seconds\n",
      "\tSample size: 2000, Accuracy: 0.46, Execution time: 30.9654 seconds\n",
      "\tSample size: 5000, Accuracy: 0.55, Execution time: 80.3204 seconds\n",
      "\tSample size: 10000, Accuracy: 0.61, Execution time: 160.4385 seconds\n",
      "Reduction Dimension: 256\n",
      "\tSample size: 100, Accuracy: 0.37, Execution time: 1.5954 seconds\n",
      "\tSample size: 500, Accuracy: 0.37, Execution time: 7.7693 seconds\n",
      "\tSample size: 1000, Accuracy: 0.41, Execution time: 15.3199 seconds\n",
      "\tSample size: 2000, Accuracy: 0.46, Execution time: 30.9338 seconds\n",
      "\tSample size: 5000, Accuracy: 0.58, Execution time: 80.7308 seconds\n",
      "\tSample size: 10000, Accuracy: 0.59, Execution time: 158.6613 seconds\n",
      "Reduction Dimension: 512\n",
      "\tSample size: 100, Accuracy: 0.23, Execution time: 1.5728 seconds\n",
      "\tSample size: 500, Accuracy: 0.38, Execution time: 7.8177 seconds\n",
      "\tSample size: 1000, Accuracy: 0.45, Execution time: 15.5350 seconds\n",
      "\tSample size: 2000, Accuracy: 0.51, Execution time: 31.2563 seconds\n",
      "\tSample size: 5000, Accuracy: 0.58, Execution time: 80.9455 seconds\n",
      "\tSample size: 10000, Accuracy: 0.60, Execution time: 157.5542 seconds\n",
      "Reduction Dimension: 784\n",
      "\tSample size: 100, Accuracy: 0.24, Execution time: 1.5961 seconds\n",
      "\tSample size: 500, Accuracy: 0.39, Execution time: 7.9833 seconds\n",
      "\tSample size: 1000, Accuracy: 0.40, Execution time: 15.8044 seconds\n",
      "\tSample size: 2000, Accuracy: 0.44, Execution time: 31.6045 seconds\n",
      "\tSample size: 5000, Accuracy: 0.57, Execution time: 80.2692 seconds\n",
      "\tSample size: 10000, Accuracy: 0.61, Execution time: 159.5667 seconds\n"
     ]
    }
   ],
   "source": [
    "# train set is sampled using M/10 prototypes per class\n",
    "\n",
    "# sample_sizes = [10, 20, 30, 40, 50]\n",
    "reduced_dims = [3, 4, 8, 16, 64, 128, 256, 512, 784]\n",
    "sample_sizes = [100, 500, 1000, 2000, 5000, 10000]\n",
    "storage = {} \n",
    "execution_data = {d:[] for d in reduced_dims}\n",
    "\n",
    "for dim in reduced_dims:\n",
    "    print('Reduction Dimension:', dim)\n",
    "    for M in sample_sizes:\n",
    "        # Sample Prototypes\n",
    "        selected_idxs = select_k_centers_per_class(x_train_scaled, y_train, k_per_class= int(M/10), n_components=dim)\n",
    "        x_sample, y_sample = x_train_scaled[selected_idxs], y_train[selected_idxs]\n",
    "\n",
    "        # Model\n",
    "        model = oneNNClassifier(x_sample, y_sample)\n",
    "\n",
    "        # Timing\n",
    "        elapsed_time = timeit.timeit(lambda: model.predict(x_test, size=M, storage=storage, weighted=False), \n",
    "                                number=1)\n",
    "        # Accuracy\n",
    "        accuracy = accuracy_score(y_test, storage[M])\n",
    "\n",
    "        print(f\"\\tSample size: {M}, Accuracy: {accuracy:.2f}, Execution time: {elapsed_time:.4f} seconds\")\n",
    "        # Store Data\n",
    "        execution_data[dim].append({\"sample_size\": M, \"time\": elapsed_time, \"accuracy\": accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"execution_data_farthest_point_clustering_scaled.json\", \"w\") as file:\n",
    "    json.dump(execution_data, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
